{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17727739-2a8a-4132-a526-443e8f7d4a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# LAI/FAPAR Date Drill-Down\n",
    "Pick a date, render a spatial map for that day. Adjust cell size and variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d154b691-e98a-4396-b651-d691bc6345cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Widgets\n",
    "dbutils.widgets.text(\"SECRET_SCOPE\", \"climate-scope\", \"Secret Scope\")\n",
    "dbutils.widgets.text(\"AZURE_ACCOUNT_KEY_NAME\", \"AZURE_STORAGE_KEY\", \"Key Secret Name\")\n",
    "dbutils.widgets.text(\"AZURE_ACCOUNT_NAME_NAME\", \"AZURE_STORAGE_ACCOUNT\", \"Account Secret Name\")\n",
    "dbutils.widgets.text(\"CONTAINER\", \"climate-data-analysis\", \"ADLS Container\")\n",
    "dbutils.widgets.text(\"BASE_DIR\", \"climate_data/*/*.parquet\", \"Base Dir/Glob\")\n",
    "dbutils.widgets.dropdown(\"SOURCE\", \"adls\", [\"adls\",\"local\"], \"Read From\")\n",
    "dbutils.widgets.text(\"PARQUET_LOCAL_GLOB\", \"ClimateRecords/parquet_output/*/*.parquet\", \"Local Parquet Glob\")\n",
    "\n",
    "dbutils.widgets.text(\"DATE\", \"2010-06-15\", \"Date (YYYY-MM-DD)\")\n",
    "dbutils.widgets.text(\"GRID_DEG\", \"0.10\", \"Spatial Cell Size (deg)\")\n",
    "dbutils.widgets.dropdown(\"VARIABLE\", \"LAI\", [\"LAI\",\"FAPAR\"], \"Variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7733bdf4-af8f-48e9-ac22-e057cb402ee3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records on 1982-01-10: 1200\n+----------+\n|null_fapar|\n+----------+\n|      1150|\n+----------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>time</th><th>latitude</th><th>longitude</th><th>fapar</th></tr></thead><tbody><tr><td>1982-01-10T00:00:00Z</td><td>26.4749984741</td><td>-81.4749984741</td><td>null</td></tr><tr><td>1982-01-10T00:00:00Z</td><td>26.4749984741</td><td>-81.4749984741</td><td>null</td></tr><tr><td>1982-01-10T00:00:00Z</td><td>26.4749984741</td><td>-81.4249954224</td><td>null</td></tr><tr><td>1982-01-10T00:00:00Z</td><td>26.4749984741</td><td>-81.4249954224</td><td>null</td></tr><tr><td>1982-01-10T00:00:00Z</td><td>26.4749984741</td><td>-81.375</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1982-01-10T00:00:00Z",
         26.4749984741,
         -81.4749984741,
         null
        ],
        [
         "1982-01-10T00:00:00Z",
         26.4749984741,
         -81.4749984741,
         null
        ],
        [
         "1982-01-10T00:00:00Z",
         26.4749984741,
         -81.4249954224,
         null
        ],
        [
         "1982-01-10T00:00:00Z",
         26.4749984741,
         -81.4249954224,
         null
        ],
        [
         "1982-01-10T00:00:00Z",
         26.4749984741,
         -81.375,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "time",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "latitude",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "longitude",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "fapar",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup & read\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "SCOPE   = dbutils.widgets.get(\"SECRET_SCOPE\")\n",
    "ACC_SEC = dbutils.widgets.get(\"AZURE_ACCOUNT_NAME_NAME\")\n",
    "KEY_SEC = dbutils.widgets.get(\"AZURE_ACCOUNT_KEY_NAME\")\n",
    "account = dbutils.secrets.get(SCOPE, ACC_SEC)\n",
    "key     = dbutils.secrets.get(SCOPE, KEY_SEC)\n",
    "spark.conf.set(f\"fs.azure.account.key.{account}.dfs.core.windows.net\", key)\n",
    "\n",
    "CONTAINER = dbutils.widgets.get(\"CONTAINER\")\n",
    "BASE_DIR  = dbutils.widgets.get(\"BASE_DIR\").strip(\"/\")\n",
    "SOURCE    = dbutils.widgets.get(\"SOURCE\")\n",
    "LOCAL_GLOB = dbutils.widgets.get(\"PARQUET_LOCAL_GLOB\")\n",
    "DATE     = dbutils.widgets.get(\"DATE\")\n",
    "GRID_DEG = float(dbutils.widgets.get(\"GRID_DEG\"))\n",
    "VAR      = dbutils.widgets.get(\"VARIABLE\")  # \"LAI\" or \"FAPAR\"\n",
    "\n",
    "path = f\"abfss://{CONTAINER}@{account}.dfs.core.windows.net/{BASE_DIR}\" if SOURCE==\"adls\" else LOCAL_GLOB\n",
    "\n",
    "# Read only the columns we need, per your schema\n",
    "raw = spark.read.parquet(path).select(\"latitude\", \"longitude\", \"time\", \"LAI\", \"FAPAR\")\n",
    "\n",
    "# Convert epoch milliseconds â†’ timestamp; also guard if any rows happen to be seconds\n",
    "secs = F.when(F.col(\"time\") > F.lit(100_000_000_000), F.col(\"time\")/F.lit(1000.0)) \\\n",
    "        .otherwise(F.col(\"time\").cast(\"double\"))\n",
    "sdf = (\n",
    "    raw\n",
    "    .withColumn(\"time\", F.to_timestamp(F.from_unixtime(secs)))\n",
    "    .filter(F.col(\"time\").isNotNull())\n",
    "    # OPTIONAL: drop early years if you want (uncomment next line)\n",
    "    # .filter(F.year(\"time\") >= 1982)\n",
    "    .withColumnRenamed(\"LAI\", \"lai\")\n",
    "    .withColumnRenamed(\"FAPAR\", \"fapar\")\n",
    ")\n",
    "\n",
    "# Ensure requested variable exists after normalization\n",
    "var_col = VAR.lower()  # \"lai\" or \"fapar\"\n",
    "\n",
    "# Filter to the requested date (YYYY-MM-DD)\n",
    "day_sdf = sdf.where(F.to_date(\"time\") == F.lit(DATE))\n",
    "\n",
    "# Quick sanity checks\n",
    "print(f\"Records on {DATE}: {day_sdf.count()}\")\n",
    "day_sdf.agg(\n",
    "    F.count(F.when(F.col(var_col).isNull(), 1)).alias(f\"null_{var_col}\")\n",
    ").show()\n",
    "\n",
    "display(day_sdf.select(\"time\",\"latitude\",\"longitude\",var_col).limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c7d8849-851c-4301-b8b5-7f2e34ec0f59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mSparkException\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5352098279229282>, line 12\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m cell \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mlit(GRID_DEG)\n",
       "\u001B[1;32m      3\u001B[0m grid_sdf \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[1;32m      4\u001B[0m     day_sdf\n",
       "\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlat_bin\u001B[39m\u001B[38;5;124m\"\u001B[39m, F\u001B[38;5;241m.\u001B[39mfloor(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlatitude\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m/\u001B[39mcell)\u001B[38;5;241m*\u001B[39mcell)\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39mselect(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlat_bin\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlat\u001B[39m\u001B[38;5;124m\"\u001B[39m), F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlon_bin\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlon\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     10\u001B[0m )\n",
       "\u001B[0;32m---> 12\u001B[0m grid \u001B[38;5;241m=\u001B[39m grid_sdf\u001B[38;5;241m.\u001B[39mtoPandas()\n",
       "\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m grid\u001B[38;5;241m.\u001B[39mempty:\n",
       "\u001B[1;32m     15\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo data for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mDATE\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Try another date or widen your selection.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/dataframe.py:2011\u001B[0m, in \u001B[0;36mDataFrame.toPandas\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   2009\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtoPandas\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpandas.DataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   2010\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 2011\u001B[0m     pdf, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mto_pandas(query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mobservations)\n",
       "\u001B[1;32m   2012\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execution_info \u001B[38;5;241m=\u001B[39m ei\n",
       "\u001B[1;32m   2013\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pdf\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1277\u001B[0m, in \u001B[0;36mSparkConnectClient.to_pandas\u001B[0;34m(self, plan, observations)\u001B[0m\n",
       "\u001B[1;32m   1273\u001B[0m (self_destruct_conf,) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_config_with_defaults(\n",
       "\u001B[1;32m   1274\u001B[0m     (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.execution.arrow.pyspark.selfDestruct.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m   1275\u001B[0m )\n",
       "\u001B[1;32m   1276\u001B[0m self_destruct \u001B[38;5;241m=\u001B[39m cast(\u001B[38;5;28mstr\u001B[39m, self_destruct_conf)\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m-> 1277\u001B[0m table, schema, metrics, observed_metrics, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1278\u001B[0m     req, observations, self_destruct\u001B[38;5;241m=\u001B[39mself_destruct\n",
       "\u001B[1;32m   1279\u001B[0m )\n",
       "\u001B[1;32m   1280\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m table \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   1281\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1973\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   1970\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   1972\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 1973\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   1974\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   1975\u001B[0m     ):\n",
       "\u001B[1;32m   1976\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   1977\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1949\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   1947\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   1948\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1949\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2269\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2267\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2268\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2269\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2270\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   2271\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2352\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2348\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2350\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2352\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2353\u001B[0m                 info,\n",
       "\u001B[1;32m   2354\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2355\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2356\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2357\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2359\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2360\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2361\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2362\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2363\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mSparkException\u001B[0m: Job aborted due to stage failure: Task 0 in stage 83.0 failed 4 times, most recent failure: Lost task 0.3 in stage 83.0 (TID 2807) (10.139.64.4 executor driver): com.databricks.sql.io.FileReadException: Error while reading file abfss:REDACTED_LOCAL_PART@[REDACTED].dfs.core.windows.net/climate_data/2009/CR20090121.parquet. com.databricks.sql.io.NativeIOException\n",
       "\tat com.databricks.sql.io.FileReadException$.fileReadError(FileReadException.scala:48)\n",
       "\tat com.databricks.sql.io.FileReadException.fileReadError(FileReadException.scala)\n",
       "\tat 0xc411692 <photon>.RecordMissingOrCorruptFile(external/workspace_spark_3_5/photon/jni-wrappers/jni-io-broker.cc:128)\n",
       "\tat 0x714cb14 <photon>.TryAndMaybeSkipFileOnError(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:281)\n",
       "\tat 0x69e69b1 <photon>.CreateAndOpenFileReaders(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:403)\n",
       "\tat 0x69e69b1 <photon>.OpenFileBatch(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:572)\n",
       "\tat 0x69e415f <photon>.DoOpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:555)\n",
       "\tat 0x69e3c29 <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:454)\n",
       "\tat 0x692d71c <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/project-node.cc:68)\n",
       "\tat 0x692d71c <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/grouping-agg-node.cc:102)\n",
       "\tat 0x692d71c <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/shuffle-sink-node.cc:171)\n",
       "\tat com.databricks.photon.JniApiImpl.open(Native Method)\n",
       "\tat com.databricks.photon.JniApi.open(JniApi.scala)\n",
       "\tat com.databricks.photon.JniExecNode.open(JniExecNode.java:73)\n",
       "\tat com.databricks.photon.PhotonPreShuffleResultHandler.$anonfun$getResult$1(PhotonExec.scala:1045)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\n",
       "\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\n",
       "\tat com.databricks.photon.PhotonPreShuffleResultHandler.timeit(PhotonExec.scala:1038)\n",
       "\tat com.databricks.photon.PhotonPreShuffleResultHandler.getResult(PhotonExec.scala:1045)\n",
       "\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)\n",
       "\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)\n",
       "\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)\n",
       "\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)\n",
       "\tat com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat com.databricks.photon.MetadataOnlyShuffleWriter.write(MetadataOnlyShuffleWriter.scala:50)\n",
       "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:56)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:87)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:82)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:58)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:39)\n",
       "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:105)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1228)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1232)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1084)\n",
       "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "Caused by: com.databricks.sql.io.NativeIOException: FAILED_PRECONDITION: HTTP/1.1 412 The condition specified using HTTP conditional header(s) is not met.\r\n",
       "\r\n",
       "{\"error\":{\"code\":\"ConditionNotMet\",\"message\":\"The condition specified using HTTP conditional header(s) is not met.\\nRequestId:b00f43dd-b01f-0067-2d13-3db31c000000\\nTime:2025-10-14T14:02:10.2739764Z\"}}\n",
       "\tat com.databricks.sql.io.NativeIOResponse.throwError(NativeStorage.java:1823)\n",
       "\tat com.databricks.sql.io.NativeFuture.get(Native Method)\n",
       "\tat com.databricks.sql.io.NativeFuture.get(NativeStorage.java:2025)\n",
       "\tat com.databricks.sql.io.NativeFuture.get(NativeStorage.java:1910)\n",
       "\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n",
       "\tat com.databricks.sql.io.PendingFutures$SelfCleaningFuture.get(PendingFutures.java:132)\n",
       "\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n",
       "\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n",
       "\tat com.databricks.sql.io.PendingFutures$SelfCleaningFuture.get(PendingFutures.java:132)\n",
       "\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n",
       "\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n",
       "\tat com.databricks.photon.NativeIOBroker$TaggedIOFutureContainer.getAndApply(NativeIOBroker.java:205)\n",
       "\tat com.databricks.photon.NativeIOBroker$TaggedIOFutureContainer.getAndApply(NativeIOBroker.java:216)\n",
       "\tat com.databricks.photon.NativeIOBroker.lambda$consumeReadRangeRequest$5(NativeIOBroker.java:470)\n",
       "\tat com.databricks.photon.NativeIOBroker.withNonPhotonBillingIfCloudStorageRequestRetried(NativeIOBroker.java:727)\n",
       "\tat com.databricks.photon.NativeIOBroker.consumeReadRangeRequest(NativeIOBroker.java:469)\n",
       "\t... 48 more\n",
       "\n",
       "Driver stacktrace:\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.SparkException\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4612)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4610)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4522)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4509)\n",
       "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
       "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
       "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4509)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4498)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2(DAGScheduler.scala:1963)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2$adapted(DAGScheduler.scala:1946)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1946)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1946)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4876)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4774)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4773)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4759)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:352)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$.org$apache$spark$sql$execution$SparkPlan$$withExecuteQueryLogging(SparkPlan.scala:137)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:417)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:413)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:348)\n",
       "\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.$anonfun$processAsArrowBatches$2(SparkConnectPlanExecution.scala:298)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:481)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:417)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:798)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:347)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1463)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:216)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:735)\n",
       "\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.processAsArrowBatches(SparkConnectPlanExecution.scala:274)\n",
       "\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:131)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:404)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:313)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1463)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
       "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SparkException",
        "evalue": "Job aborted due to stage failure: Task 0 in stage 83.0 failed 4 times, most recent failure: Lost task 0.3 in stage 83.0 (TID 2807) (10.139.64.4 executor driver): com.databricks.sql.io.FileReadException: Error while reading file abfss:REDACTED_LOCAL_PART@datalakeq7aj6k0.dfs.core.windows.net/climate_data/2009/CR20090121.parquet. com.databricks.sql.io.NativeIOException\n\tat com.databricks.sql.io.FileReadException$.fileReadError(FileReadException.scala:48)\n\tat com.databricks.sql.io.FileReadException.fileReadError(FileReadException.scala)\n\tat 0xc411692 <photon>.RecordMissingOrCorruptFile(external/workspace_spark_3_5/photon/jni-wrappers/jni-io-broker.cc:128)\n\tat 0x714cb14 <photon>.TryAndMaybeSkipFileOnError(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:281)\n\tat 0x69e69b1 <photon>.CreateAndOpenFileReaders(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:403)\n\tat 0x69e69b1 <photon>.OpenFileBatch(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:572)\n\tat 0x69e415f <photon>.DoOpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:555)\n\tat 0x69e3c29 <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:454)\n\tat 0x692d71c <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/project-node.cc:68)\n\tat 0x692d71c <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/grouping-agg-node.cc:102)\n\tat 0x692d71c <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/shuffle-sink-node.cc:171)\n\tat com.databricks.photon.JniApiImpl.open(Native Method)\n\tat com.databricks.photon.JniApi.open(JniApi.scala)\n\tat com.databricks.photon.JniExecNode.open(JniExecNode.java:73)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.$anonfun$getResult$1(PhotonExec.scala:1045)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\n\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.timeit(PhotonExec.scala:1038)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.getResult(PhotonExec.scala:1045)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)\n\tat com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat com.databricks.photon.MetadataOnlyShuffleWriter.write(MetadataOnlyShuffleWriter.scala:50)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:56)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:87)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:58)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:39)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:105)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1228)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1232)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1084)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.databricks.sql.io.NativeIOException: FAILED_PRECONDITION: HTTP/1.1 412 The condition specified using HTTP conditional header(s) is not met.\r\n\r\n{\"error\":{\"code\":\"ConditionNotMet\",\"message\":\"The condition specified using HTTP conditional header(s) is not met.\\nRequestId:b00f43dd-b01f-0067-2d13-3db31c000000\\nTime:2025-10-14T14:02:10.2739764Z\"}}\n\tat com.databricks.sql.io.NativeIOResponse.throwError(NativeStorage.java:1823)\n\tat com.databricks.sql.io.NativeFuture.get(Native Method)\n\tat com.databricks.sql.io.NativeFuture.get(NativeStorage.java:2025)\n\tat com.databricks.sql.io.NativeFuture.get(NativeStorage.java:1910)\n\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n\tat com.databricks.sql.io.PendingFutures$SelfCleaningFuture.get(PendingFutures.java:132)\n\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n\tat com.databricks.sql.io.PendingFutures$SelfCleaningFuture.get(PendingFutures.java:132)\n\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n\tat com.databricks.photon.NativeIOBroker$TaggedIOFutureContainer.getAndApply(NativeIOBroker.java:205)\n\tat com.databricks.photon.NativeIOBroker$TaggedIOFutureContainer.getAndApply(NativeIOBroker.java:216)\n\tat com.databricks.photon.NativeIOBroker.lambda$consumeReadRangeRequest$5(NativeIOBroker.java:470)\n\tat com.databricks.photon.NativeIOBroker.withNonPhotonBillingIfCloudStorageRequestRetried(NativeIOBroker.java:727)\n\tat com.databricks.photon.NativeIOBroker.consumeReadRangeRequest(NativeIOBroker.java:469)\n\t... 48 more\n\nDriver stacktrace:\n\nJVM stacktrace:\norg.apache.spark.SparkException\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4612)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4610)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4522)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4509)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4509)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4498)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2(DAGScheduler.scala:1963)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2$adapted(DAGScheduler.scala:1946)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1946)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1946)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4876)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4774)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4773)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4759)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:352)\n\tat org.apache.spark.sql.execution.SparkPlan$.org$apache$spark$sql$execution$SparkPlan$$withExecuteQueryLogging(SparkPlan.scala:137)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:417)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:413)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:348)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.$anonfun$processAsArrowBatches$2(SparkConnectPlanExecution.scala:298)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:481)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:417)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:798)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:347)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1463)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:216)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:735)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.processAsArrowBatches(SparkConnectPlanExecution.scala:274)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:131)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:404)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:313)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1463)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>SparkException</span>: Job aborted due to stage failure: Task 0 in stage 83.0 failed 4 times, most recent failure: Lost task 0.3 in stage 83.0 (TID 2807) (10.139.64.4 executor driver): com.databricks.sql.io.FileReadException: Error while reading file abfss:REDACTED_LOCAL_PART@[REDACTED].dfs.core.windows.net/climate_data/2009/CR20090121.parquet. com.databricks.sql.io.NativeIOException\n\tat com.databricks.sql.io.FileReadException$.fileReadError(FileReadException.scala:48)\n\tat com.databricks.sql.io.FileReadException.fileReadError(FileReadException.scala)\n\tat 0xc411692 <photon>.RecordMissingOrCorruptFile(external/workspace_spark_3_5/photon/jni-wrappers/jni-io-broker.cc:128)\n\tat 0x714cb14 <photon>.TryAndMaybeSkipFileOnError(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:281)\n\tat 0x69e69b1 <photon>.CreateAndOpenFileReaders(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:403)\n\tat 0x69e69b1 <photon>.OpenFileBatch(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:572)\n\tat 0x69e415f <photon>.DoOpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:555)\n\tat 0x69e3c29 <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:454)\n\tat 0x692d71c <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/project-node.cc:68)\n\tat 0x692d71c <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/grouping-agg-node.cc:102)\n\tat 0x692d71c <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/shuffle-sink-node.cc:171)\n\tat com.databricks.photon.JniApiImpl.open(Native Method)\n\tat com.databricks.photon.JniApi.open(JniApi.scala)\n\tat com.databricks.photon.JniExecNode.open(JniExecNode.java:73)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.$anonfun$getResult$1(PhotonExec.scala:1045)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\n\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.timeit(PhotonExec.scala:1038)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.getResult(PhotonExec.scala:1045)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)\n\tat com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat com.databricks.photon.MetadataOnlyShuffleWriter.write(MetadataOnlyShuffleWriter.scala:50)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:56)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:87)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:58)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:39)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:105)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1228)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1232)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1084)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.databricks.sql.io.NativeIOException: FAILED_PRECONDITION: HTTP/1.1 412 The condition specified using HTTP conditional header(s) is not met.\r\n\r\n{\"error\":{\"code\":\"ConditionNotMet\",\"message\":\"The condition specified using HTTP conditional header(s) is not met.\\nRequestId:b00f43dd-b01f-0067-2d13-3db31c000000\\nTime:2025-10-14T14:02:10.2739764Z\"}}\n\tat com.databricks.sql.io.NativeIOResponse.throwError(NativeStorage.java:1823)\n\tat com.databricks.sql.io.NativeFuture.get(Native Method)\n\tat com.databricks.sql.io.NativeFuture.get(NativeStorage.java:2025)\n\tat com.databricks.sql.io.NativeFuture.get(NativeStorage.java:1910)\n\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n\tat com.databricks.sql.io.PendingFutures$SelfCleaningFuture.get(PendingFutures.java:132)\n\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n\tat com.databricks.sql.io.PendingFutures$SelfCleaningFuture.get(PendingFutures.java:132)\n\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n\tat com.databricks.photon.NativeIOBroker$TaggedIOFutureContainer.getAndApply(NativeIOBroker.java:205)\n\tat com.databricks.photon.NativeIOBroker$TaggedIOFutureContainer.getAndApply(NativeIOBroker.java:216)\n\tat com.databricks.photon.NativeIOBroker.lambda$consumeReadRangeRequest$5(NativeIOBroker.java:470)\n\tat com.databricks.photon.NativeIOBroker.withNonPhotonBillingIfCloudStorageRequestRetried(NativeIOBroker.java:727)\n\tat com.databricks.photon.NativeIOBroker.consumeReadRangeRequest(NativeIOBroker.java:469)\n\t... 48 more\n\nDriver stacktrace:\n\nJVM stacktrace:\norg.apache.spark.SparkException\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4612)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4610)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4522)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4509)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4509)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4498)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2(DAGScheduler.scala:1963)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2$adapted(DAGScheduler.scala:1946)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1946)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1946)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4876)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4774)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4773)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4759)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:352)\n\tat org.apache.spark.sql.execution.SparkPlan$.org$apache$spark$sql$execution$SparkPlan$$withExecuteQueryLogging(SparkPlan.scala:137)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:417)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:413)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:348)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.$anonfun$processAsArrowBatches$2(SparkConnectPlanExecution.scala:298)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:481)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:417)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:798)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:347)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1463)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:216)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:735)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.processAsArrowBatches(SparkConnectPlanExecution.scala:274)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:131)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:404)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:313)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1463)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": null,
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": "XXKCM",
        "stackTrace": "org.apache.spark.SparkException\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4612)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4610)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4522)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4509)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4509)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4498)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2(DAGScheduler.scala:1963)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2$adapted(DAGScheduler.scala:1946)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1946)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1946)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4876)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4774)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4773)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4759)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:352)\n\tat org.apache.spark.sql.execution.SparkPlan$.org$apache$spark$sql$execution$SparkPlan$$withExecuteQueryLogging(SparkPlan.scala:137)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:417)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:413)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:348)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.$anonfun$processAsArrowBatches$2(SparkConnectPlanExecution.scala:298)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:481)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:417)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:798)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:347)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1463)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:216)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:735)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.processAsArrowBatches(SparkConnectPlanExecution.scala:274)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:131)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:404)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:313)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1463)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mSparkException\u001B[0m                            Traceback (most recent call last)",
        "File \u001B[0;32m<command-5352098279229282>, line 12\u001B[0m\n\u001B[1;32m      2\u001B[0m cell \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mlit(GRID_DEG)\n\u001B[1;32m      3\u001B[0m grid_sdf \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      4\u001B[0m     day_sdf\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlat_bin\u001B[39m\u001B[38;5;124m\"\u001B[39m, F\u001B[38;5;241m.\u001B[39mfloor(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlatitude\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m/\u001B[39mcell)\u001B[38;5;241m*\u001B[39mcell)\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39mselect(F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlat_bin\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlat\u001B[39m\u001B[38;5;124m\"\u001B[39m), F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlon_bin\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlon\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     10\u001B[0m )\n\u001B[0;32m---> 12\u001B[0m grid \u001B[38;5;241m=\u001B[39m grid_sdf\u001B[38;5;241m.\u001B[39mtoPandas()\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m grid\u001B[38;5;241m.\u001B[39mempty:\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo data for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mDATE\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Try another date or widen your selection.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/dataframe.py:2011\u001B[0m, in \u001B[0;36mDataFrame.toPandas\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2009\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtoPandas\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpandas.DataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   2010\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 2011\u001B[0m     pdf, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mto_pandas(query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mobservations)\n\u001B[1;32m   2012\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execution_info \u001B[38;5;241m=\u001B[39m ei\n\u001B[1;32m   2013\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pdf\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1277\u001B[0m, in \u001B[0;36mSparkConnectClient.to_pandas\u001B[0;34m(self, plan, observations)\u001B[0m\n\u001B[1;32m   1273\u001B[0m (self_destruct_conf,) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_config_with_defaults(\n\u001B[1;32m   1274\u001B[0m     (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.execution.arrow.pyspark.selfDestruct.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1275\u001B[0m )\n\u001B[1;32m   1276\u001B[0m self_destruct \u001B[38;5;241m=\u001B[39m cast(\u001B[38;5;28mstr\u001B[39m, self_destruct_conf)\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1277\u001B[0m table, schema, metrics, observed_metrics, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1278\u001B[0m     req, observations, self_destruct\u001B[38;5;241m=\u001B[39mself_destruct\n\u001B[1;32m   1279\u001B[0m )\n\u001B[1;32m   1280\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m table \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1281\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1973\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   1970\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1972\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1973\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   1974\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   1975\u001B[0m     ):\n\u001B[1;32m   1976\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1977\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1949\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   1947\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   1948\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1949\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2269\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2267\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2268\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2269\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2270\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2271\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2352\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2348\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2350\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2352\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2353\u001B[0m                 info,\n\u001B[1;32m   2354\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2355\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2356\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2357\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2359\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2360\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2361\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2362\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2363\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mSparkException\u001B[0m: Job aborted due to stage failure: Task 0 in stage 83.0 failed 4 times, most recent failure: Lost task 0.3 in stage 83.0 (TID 2807) (10.139.64.4 executor driver): com.databricks.sql.io.FileReadException: Error while reading file abfss:REDACTED_LOCAL_PART@[REDACTED].dfs.core.windows.net/climate_data/2009/CR20090121.parquet. com.databricks.sql.io.NativeIOException\n\tat com.databricks.sql.io.FileReadException$.fileReadError(FileReadException.scala:48)\n\tat com.databricks.sql.io.FileReadException.fileReadError(FileReadException.scala)\n\tat 0xc411692 <photon>.RecordMissingOrCorruptFile(external/workspace_spark_3_5/photon/jni-wrappers/jni-io-broker.cc:128)\n\tat 0x714cb14 <photon>.TryAndMaybeSkipFileOnError(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:281)\n\tat 0x69e69b1 <photon>.CreateAndOpenFileReaders(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:403)\n\tat 0x69e69b1 <photon>.OpenFileBatch(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:572)\n\tat 0x69e415f <photon>.DoOpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:555)\n\tat 0x69e3c29 <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:454)\n\tat 0x692d71c <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/project-node.cc:68)\n\tat 0x692d71c <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/grouping-agg-node.cc:102)\n\tat 0x692d71c <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/shuffle-sink-node.cc:171)\n\tat com.databricks.photon.JniApiImpl.open(Native Method)\n\tat com.databricks.photon.JniApi.open(JniApi.scala)\n\tat com.databricks.photon.JniExecNode.open(JniExecNode.java:73)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.$anonfun$getResult$1(PhotonExec.scala:1045)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\n\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.timeit(PhotonExec.scala:1038)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.getResult(PhotonExec.scala:1045)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)\n\tat com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat com.databricks.photon.MetadataOnlyShuffleWriter.write(MetadataOnlyShuffleWriter.scala:50)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:56)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:87)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:58)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:39)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:105)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1228)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1232)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1084)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.databricks.sql.io.NativeIOException: FAILED_PRECONDITION: HTTP/1.1 412 The condition specified using HTTP conditional header(s) is not met.\r\n\r\n{\"error\":{\"code\":\"ConditionNotMet\",\"message\":\"The condition specified using HTTP conditional header(s) is not met.\\nRequestId:b00f43dd-b01f-0067-2d13-3db31c000000\\nTime:2025-10-14T14:02:10.2739764Z\"}}\n\tat com.databricks.sql.io.NativeIOResponse.throwError(NativeStorage.java:1823)\n\tat com.databricks.sql.io.NativeFuture.get(Native Method)\n\tat com.databricks.sql.io.NativeFuture.get(NativeStorage.java:2025)\n\tat com.databricks.sql.io.NativeFuture.get(NativeStorage.java:1910)\n\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n\tat com.databricks.sql.io.PendingFutures$SelfCleaningFuture.get(PendingFutures.java:132)\n\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n\tat com.databricks.sql.io.PendingFutures$SelfCleaningFuture.get(PendingFutures.java:132)\n\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n\tat com.databricks.sql.io.Futures$4.get(Futures.java:144)\n\tat com.databricks.photon.NativeIOBroker$TaggedIOFutureContainer.getAndApply(NativeIOBroker.java:205)\n\tat com.databricks.photon.NativeIOBroker$TaggedIOFutureContainer.getAndApply(NativeIOBroker.java:216)\n\tat com.databricks.photon.NativeIOBroker.lambda$consumeReadRangeRequest$5(NativeIOBroker.java:470)\n\tat com.databricks.photon.NativeIOBroker.withNonPhotonBillingIfCloudStorageRequestRetried(NativeIOBroker.java:727)\n\tat com.databricks.photon.NativeIOBroker.consumeReadRangeRequest(NativeIOBroker.java:469)\n\t... 48 more\n\nDriver stacktrace:\n\nJVM stacktrace:\norg.apache.spark.SparkException\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$failJobAndIndependentStages$1(DAGScheduler.scala:4612)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:4610)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4(DAGScheduler.scala:4522)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$4$adapted(DAGScheduler.scala:4509)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$1(DAGScheduler.scala:4509)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:4498)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2(DAGScheduler.scala:1963)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$2$adapted(DAGScheduler.scala:1946)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1946)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1946)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:4876)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.liftedTree1$1(DAGScheduler.scala:4774)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4773)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:4759)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:55)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:352)\n\tat org.apache.spark.sql.execution.SparkPlan$.org$apache$spark$sql$execution$SparkPlan$$withExecuteQueryLogging(SparkPlan.scala:137)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:417)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:413)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:348)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.$anonfun$processAsArrowBatches$2(SparkConnectPlanExecution.scala:298)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:481)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:417)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:798)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:347)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1463)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:216)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:735)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.processAsArrowBatches(SparkConnectPlanExecution.scala:274)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:131)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:404)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:313)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1463)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spatial hexbin for the selected day\n",
    "cell = F.lit(GRID_DEG)\n",
    "grid_sdf = (\n",
    "    day_sdf\n",
    "    .withColumn(\"lat_bin\", F.floor(F.col(\"latitude\")/cell)*cell)\n",
    "    .withColumn(\"lon_bin\", F.floor(F.col(\"longitude\")/cell)*cell)\n",
    "    .groupBy(\"lat_bin\",\"lon_bin\")\n",
    "    .agg(F.mean(var_col).alias(\"value\"))\n",
    "    .select(F.col(\"lat_bin\").alias(\"lat\"), F.col(\"lon_bin\").alias(\"lon\"), \"value\")\n",
    ")\n",
    "\n",
    "grid = grid_sdf.toPandas()\n",
    "\n",
    "if grid.empty:\n",
    "    print(f\"No data for {DATE}. Try another date or widen your selection.\")\n",
    "else:\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.hexbin(grid[\"lon\"], grid[\"lat\"], C=grid[\"value\"], gridsize=40)\n",
    "    plt.title(f\"{VAR} â€” {DATE} (cell {GRID_DEG}Â°)\")\n",
    "    plt.xlabel(\"Longitude\"); plt.ylabel(\"Latitude\")\n",
    "    plt.colorbar(label=VAR)\n",
    "    plt.tight_layout()\n",
    "    display(plt.gcf()); plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_lai_fapar_drilldown",
   "widgets": {
    "AZURE_ACCOUNT_KEY_NAME": {
     "currentValue": "AZURE_STORAGE_KEY",
     "nuid": "868fe37b-3446-4209-bfdb-af1d255e245f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "AZURE_STORAGE_KEY",
      "label": "Key Secret Name",
      "name": "AZURE_ACCOUNT_KEY_NAME",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "AZURE_STORAGE_KEY",
      "label": "Key Secret Name",
      "name": "AZURE_ACCOUNT_KEY_NAME",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "AZURE_ACCOUNT_NAME_NAME": {
     "currentValue": "AZURE_STORAGE_ACCOUNT",
     "nuid": "32b431e0-d25e-4efb-aa2d-21da9a858a95",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "AZURE_STORAGE_ACCOUNT",
      "label": "Account Secret Name",
      "name": "AZURE_ACCOUNT_NAME_NAME",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "AZURE_STORAGE_ACCOUNT",
      "label": "Account Secret Name",
      "name": "AZURE_ACCOUNT_NAME_NAME",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "BASE_DIR": {
     "currentValue": "backup_climate_data/*/*.parquet",
     "nuid": "84282c73-5929-4d25-ae0c-eee103f7d56c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "climate_data/*/*.parquet",
      "label": "Base Dir/Glob",
      "name": "BASE_DIR",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "climate_data/*/*.parquet",
      "label": "Base Dir/Glob",
      "name": "BASE_DIR",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "CONTAINER": {
     "currentValue": "climate-data-analysis",
     "nuid": "2ccac55b-9383-4713-a720-93e9946888bd",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "climate-data-analysis",
      "label": "ADLS Container",
      "name": "CONTAINER",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "climate-data-analysis",
      "label": "ADLS Container",
      "name": "CONTAINER",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "DATE": {
     "currentValue": "1982-01-10",
     "nuid": "d7ec9039-fd7f-48c2-87c7-e458ee398bbe",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2010-06-15",
      "label": "Date (YYYY-MM-DD)",
      "name": "DATE",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2010-06-15",
      "label": "Date (YYYY-MM-DD)",
      "name": "DATE",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "GRID_DEG": {
     "currentValue": "0.10",
     "nuid": "145b84f2-158c-4e9c-9970-ea45f4100b85",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "0.10",
      "label": "Spatial Cell Size (deg)",
      "name": "GRID_DEG",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0.10",
      "label": "Spatial Cell Size (deg)",
      "name": "GRID_DEG",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "PARQUET_LOCAL_GLOB": {
     "currentValue": "ClimateRecords/parquet_output/*/*.parquet",
     "nuid": "9fbd331e-8362-41e8-836c-520ed72b872a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "ClimateRecords/parquet_output/*/*.parquet",
      "label": "Local Parquet Glob",
      "name": "PARQUET_LOCAL_GLOB",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "ClimateRecords/parquet_output/*/*.parquet",
      "label": "Local Parquet Glob",
      "name": "PARQUET_LOCAL_GLOB",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "SECRET_SCOPE": {
     "currentValue": "climate-scope",
     "nuid": "8473ebc7-ef4e-4dac-a202-488a7d04aba5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "climate-scope",
      "label": "Secret Scope",
      "name": "SECRET_SCOPE",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "climate-scope",
      "label": "Secret Scope",
      "name": "SECRET_SCOPE",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "SOURCE": {
     "currentValue": "adls",
     "nuid": "f01dd6bf-8d78-463a-ad81-d22c0c860e3d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "adls",
      "label": "Read From",
      "name": "SOURCE",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "adls",
        "local"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "adls",
      "label": "Read From",
      "name": "SOURCE",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "adls",
        "local"
       ]
      }
     }
    },
    "VARIABLE": {
     "currentValue": "FAPAR",
     "nuid": "b5c5ef61-1b53-44d3-9303-783e49152b23",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "LAI",
      "label": "Variable",
      "name": "VARIABLE",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "LAI",
        "FAPAR"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "LAI",
      "label": "Variable",
      "name": "VARIABLE",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "LAI",
        "FAPAR"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}